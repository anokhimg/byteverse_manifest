version: 0.1.0
jobId: "1126"
jobName: test kk1
jobType: Source Aligned Data Product
alias: asdsadsda
discoveryPort:
  name: test kk1a
inputPorts:
  - alias: S3_Data_Set_Parquet_With_More_Variables_1
    description: S3 Data Set Parquet With More Variables
    tags: []
    extra: {}
    syncType: pull
    type: s3-parquet
    dataSetUrn: urn:dv:dataset:fd4b0582-4c8d-422b-b948-2e515dec63e2
    filter: ""
    projection: []
    persistDataFrame: false
    entity:
      advanceOptions:
        mergeSchema: true
  - alias: S3_Data_Set_CSV_1
    description: S3 Data Set CSV
    tags: []
    extra: {}
    syncType: pull
    type: s3-csv
    dataSetUrn: urn:dv:dataset:ed99e7b6-5f98-4c19-989c-5031aae66a5a
    filter: ""
    projection: []
    persistDataFrame: false
    entity:
      advanceOptions:
        mergeSchema: false
productState:
  stepName: asdsad
  inputDataFrame: Databricks_PySpark_1
  tableName: asdasd
  warehousePath: asdasdas
  catalogName: asdasdasd
  writeMode: Overwrite
  stateType: inputParquet
  stateName: asdsa
  optional:
    encodingType: HASH
    columnsToEncode:
      - aasd
    partitionColumns:
      - aasd
    primaryKey:
      - aasdas
    changeTrackingColumns:
      - aasdsa
    timestampColumn: asdasd
    persistDataFrame: false
    dataProfiling:
      enable: true
      tableId: 1
  type: stateManagement
  sequence: 4
  updateStrategy: Overwrite
  logicalSchema: {}
  refreshInterval: "* * * 2 *"
transformation:
  - stepName: asdasd
    arguments:
      - s3://normal_path
      - s3://##var1##/##var2##/test/##var3##/##var4##
    pythonFilePath: asdasd
    optional:
      dependencies:
        - aasdasd
    type: customPySparkDatabricks
    sequence: 3
controlPort:
  dataQualityRules: {}
outputPort:
  subscriptionChannels:
    - channelType: Postgres
      queryType: SQL
