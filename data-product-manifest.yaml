version: 0.1.0
jobId: "1211"
jobName: testkk1234
jobType: Source Aligned Data Product
alias: asdas
discoveryPort:
  name: testkk1234
inputPorts:
  - alias: weather_day_1
    description: weather day
    tags: []
    extra: {}
    syncType: pull
    type: s3-csv
    dataSetUrn: urn:dv:dataset:3a0494b4-86e9-430f-afcb-877a0d9bea95
    filter: ""
    projection: []
    persistDataFrame: false
    entity:
      advanceOptions:
        mergeSchema: false
  - alias: S3_Data_Set_CSV_1
    description: S3 Data Set CSV
    tags: []
    extra: {}
    syncType: pull
    type: s3-csv
    dataSetUrn: urn:dv:dataset:ed99e7b6-5f98-4c19-989c-5031aae66a5a
    filter: ""
    projection: []
    persistDataFrame: false
    entity:
      advanceOptions:
        mergeSchema: false
productState:
  stepName: asdas
  inputDataFrame: EMR_PySpark_1
  tableName: asdas
  warehousePath: asdasd
  catalogName: asdsadsa
  writeMode: Overwrite
  stateType: inputParquet
  stateName: asdas
  optional:
    encodingType: HASH
    persistDataFrame: false
  type: stateManagement
  sequence: 4
  updateStrategy: Overwrite
  refreshInterval: "* * * 2 *"
transformation:
  - stepName: kk1asd
    arguments:
      - s3://byte-etl-externaldemo/weather_data/20230626110117.csv
      - s3://normal_path
    pythonFilePath: python filepath
    inputDataFrameList:
      - inputDataFrame: weather$day$1
        tempViewName: kk1
      - inputDataFrame: S3$Data$Set$CSV$1
        tempViewName: kk2
    optional:
      pythonExtraZipFilePath: asda
      pythonEnvTarGZPath: python targz
    type: customPySparkEMRServerless
    sequence: 3
    references:
      - alias: weather_day_1
        sqlReference: kk1
      - alias: S3_Data_Set_CSV_1
        sqlReference: kk2
controlPort:
  dataQualityRules: {}
outputPort:
  subscriptionChannels:
    - channelType: Postgres
      queryType: SQL
